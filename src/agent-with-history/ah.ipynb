{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74fd2394be35f78",
   "metadata": {},
   "source": [
    "# langChain AgentWithHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffba68acc943b36",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "### Environment and APIs"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"Example LangChain server exposes and agent that has conversation history.\n",
    "\n",
    "In this example, the history is stored entirely on the client's side.\n",
    "\n",
    "Please see other examples in LangServe on how to use RunnableWithHistory to\n",
    "store history on the server side.\n",
    "\n",
    "Relevant LangChain documentation:\n",
    "\n",
    "* Creating a custom agent: https://python.langchain.com/docs/modules/agents/how_to/custom_agent\n",
    "* Streaming with agents: https://python.langchain.com/docs/modules/agents/how_to/streaming#custom-streaming-with-events\n",
    "* General streaming documentation: https://python.langchain.com/docs/expression_language/streaming\n",
    "* Message History: https://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "\n",
    "**ATTENTION**\n",
    "1. To support streaming individual tokens you will need to use the astream events\n",
    "   endpoint rather than the streaming endpoint.\n",
    "2. This example does not truncate message history, so it will crash if you\n",
    "   send too many messages (exceed token length).\n",
    "3. The playground at the moment does not render agent output well! If you want to\n",
    "   use the playground you need to customize it's output server side using astream\n",
    "   events by wrapping it within another runnable.\n",
    "4. See the client notebook it has an example of how to use stream_events client side!\n",
    "\"\"\"  # noqa: E501\n",
    "from typing import Any, List, Union\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain.agents import AgentExecutor, tool\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain_community.tools.convert_to_openai import format_tool_to_openai_tool\n",
    "from langchain_core.messages import AIMessage, FunctionMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langserve import add_routes\n",
    "from langserve.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# 0. Load Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84578baf81a56e53",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b6708cfc2f3077",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Agent\n",
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1334a2525e2ad9a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but bad at calculating lengths of words. \"\n",
    "            \"Talk with the user as normal. \"\n",
    "            \"If they ask you to calculate the length of a word, use a tool\",\n",
    "        ),\n",
    "        # Please note the ordering of the fields in the prompt!\n",
    "        # The correct ordering is:\n",
    "        # 1. history - the past messages between the user and the agent\n",
    "        # 2. user - the user's current input\n",
    "        # 3. agent_scratchpad - the agent's working space for thinking and\n",
    "        #    invoking tools to respond to the user's input.\n",
    "        # If you change the ordering, the agent will not work correctly since\n",
    "        # the messages will be shown to the underlying LLM in the wrong order.\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tools"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7aa14df2cac157c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d930132434c0f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def word_length(word: str) -> int:\n",
    "    \"\"\"Returns a counter word\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "tools = [word_length]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95c2bd2c63e557f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# We need to set streaming=True on the LLM to support streaming individual tokens.\n",
    "# Tokens will be available when using the stream_log / stream events endpoints,\n",
    "# but not when using the stream endpoint since the stream implementation for agent\n",
    "# streams action observation pairs not individual tokens.\n",
    "# See the client notebook that shows how to use the stream events endpoint.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "llm_with_tools = llm.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba3dc2ce4099db3e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agent"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81c2757736f368bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ATTENTION: For production use case, it's a good idea to trim the prompt to avoid\n",
    "#            exceeding the context window length used by the model.\n",
    "#\n",
    "# To fix that simply adjust the chain to trim the prompt in whatever way\n",
    "# is appropriate for your use case.\n",
    "# For example, you may want to keep the system message and the last 10 messages.\n",
    "# Or you may want to trim based on the number of tokens.\n",
    "# Or you may want to also summarize the messages to keep information about things\n",
    "# that were learned about the user.\n",
    "#\n",
    "# def prompt_trimmer(messages: List[Union[HumanMessage, AIMessage, FunctionMessage]]):\n",
    "#     '''Trims the prompt to a reasonable length.'''\n",
    "#     # Keep in mind that when trimming you may want to keep the system message!\n",
    "#     return messages[-10:] # Keep last 10 messages.\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    # | prompt_trimmer # See comment above.\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4647e9d02a7a2893",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Server"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b460b74ba2dfc09"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Spin up a simple api server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "\n",
    "# We need to add these input/output schemas because the current AgentExecutor\n",
    "# is lacking in schemas.\n",
    "class Input(BaseModel):\n",
    "    input: str\n",
    "    # The field extra defines a chat widget.\n",
    "    # Please see documentation about widgets in the main README.\n",
    "    # The widget is used in the playground.\n",
    "    # Keep in mind that playground support for agents is not great at the moment.\n",
    "    # To get a better experience, you'll need to customize the streaming output\n",
    "    # for now.\n",
    "    chat_history: List[Union[HumanMessage, AIMessage, FunctionMessage]] = Field(\n",
    "        ...,\n",
    "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"input\", \"output\": \"output\"}},\n",
    "    )\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    output: Any\n",
    "\n",
    "\n",
    "# Adds routes to the app for using the chain under:\n",
    "# /invoke\n",
    "# /batch\n",
    "# /stream\n",
    "# /stream_events\n",
    "add_routes(\n",
    "    app,\n",
    "    agent_executor.with_types(input_type=Input, output_type=Output).with_config(\n",
    "        {\"run_name\": \"agent\"}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "# \n",
    "#     uvicorn.run(app, host=\"localhost\", port=8000)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "444ade6b0cc7bf82",
   "execution_count": null
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": "32",
    "lenType": 16,
    "lenVar": "1024"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
